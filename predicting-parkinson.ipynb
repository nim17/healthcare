{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model,load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau,EarlyStopping\n",
    "\n",
    "from scipy.misc import toimage,imresize\n",
    "from skimage import exposure\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72 images belonging to 2 classes.\n",
      "Found 30 images belonging to 2 classes.\n",
      "Found 72 images belonging to 2 classes.\n",
      "Found 30 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "spirals_train_folder = '../input/drawings/spiral/training'\n",
    "spirals_val_folder = '../input/drawings/spiral/testing'\n",
    "waves_train_folder = '../input/drawings/wave/training'\n",
    "waves_val_folder = '../input/drawings/wave/testing'\n",
    "\n",
    "batch_size = 24\n",
    "\n",
    "# histogram equalizer\n",
    "def eqz_plz(img):\n",
    "    return exposure.equalize_hist(img)\n",
    "\n",
    "\n",
    "spiral_datagen = ImageDataGenerator(rotation_range=360, # they're spirals.\n",
    "                                    width_shift_range=0.1,\n",
    "                                    height_shift_range=0.1,\n",
    "                                    brightness_range=(0.5,1.5),\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    horizontal_flip=True,\n",
    "                                    preprocessing_function=eqz_plz,\n",
    "                                    vertical_flip=True)\n",
    "\n",
    "wave_datagen = ImageDataGenerator(rotation_range=5,\n",
    "                                  width_shift_range=0.1,\n",
    "                                  height_shift_range=0.1,\n",
    "                                  brightness_range=(0.5,1.5),\n",
    "                                  shear_range=0.2,\n",
    "                                  zoom_range=0.2,\n",
    "                                  horizontal_flip=True,\n",
    "                                  preprocessing_function=eqz_plz,\n",
    "                                  vertical_flip=True)\n",
    "\n",
    "\n",
    "spiral_train_generator = spiral_datagen.flow_from_directory(directory=os.path.abspath(spirals_train_folder),\n",
    "                                                            target_size=(256, 256),\n",
    "                                                            color_mode=\"grayscale\",\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            class_mode=\"binary\",\n",
    "                                                            shuffle=True,\n",
    "                                                            seed=666)\n",
    "\n",
    "spiral_val_generator = spiral_datagen.flow_from_directory(directory=os.path.abspath(spirals_val_folder),\n",
    "                                                            target_size=(256, 256),\n",
    "                                                            color_mode=\"grayscale\",\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            class_mode=\"binary\",\n",
    "                                                            shuffle=True,\n",
    "                                                            seed=710)\n",
    "\n",
    "wave_train_generator = wave_datagen.flow_from_directory(directory=os.path.abspath(waves_train_folder),\n",
    "                                                        target_size=(256, 512), # HxW in machine learning, WxH in computer vision\n",
    "                                                        color_mode=\"grayscale\",\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode=\"binary\",\n",
    "                                                        shuffle=True,\n",
    "                                                        seed=420)\n",
    "\n",
    "wave_val_generator = wave_datagen.flow_from_directory(directory=os.path.abspath(waves_val_folder),\n",
    "                                                        target_size=(256, 512),\n",
    "                                                        color_mode=\"grayscale\",\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode=\"binary\",\n",
    "                                                        shuffle=True,\n",
    "                                                        seed=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',patience=12,min_lr=1e-9,verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss',patience=16,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "def nopamine_model(mode):\n",
    "    if (mode == 'spirals') or (mode == 'spiral'):\n",
    "        input_layer = Input(shape=(256,256,1),name=f'{mode}_input_layer')\n",
    "    elif (mode == 'waves') or (mode == 'wave'):\n",
    "        input_layer = Input(shape=(256,512,1),name=f'{mode}_input_layer')\n",
    "\n",
    "    m1 = Conv2D(256,(5,5),dilation_rate=4,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001),activation='relu',padding='same')(input_layer)\n",
    "    p1 = MaxPool2D((9,9),strides=3)(m1)\n",
    "    m2 = Conv2D(128,(5,5),dilation_rate=2,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001),activation='relu',padding='same')(p1)\n",
    "    p2 = MaxPool2D((7,7),strides=3)(m2)\n",
    "    m3 = Conv2D(64,(3,3),kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001),activation='relu',padding='same')(p2)\n",
    "    p3 = MaxPool2D((5,5),strides=2)(m3)\n",
    "    g1=Dropout(0.2)(p3)\n",
    "    f1 = Flatten()(g1)\n",
    "    d1 = Dense(666,activation='relu')(f1)\n",
    "    g2=Dropout(0.5)(d1)\n",
    "    d2 = Dense(1,activation='sigmoid')(g2)\n",
    "    \n",
    "\n",
    "    \n",
    "    this_model = Model(input_layer,d2)\n",
    "    this_model.summary()\n",
    "    return this_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network for spirals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "spirals_input_layer (InputLa (None, 256, 256, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 256, 256, 256)     6656      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 83, 83, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 83, 83, 128)       819328    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 64)        73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 7744)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 666)               5158170   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 666)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 667       \n",
      "=================================================================\n",
      "Total params: 6,058,613\n",
      "Trainable params: 6,058,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "spiral_model = nopamine_model(mode='spirals') # early stopping epoch 89: val_loss 0.4796, val_acc 0.8274\n",
    "spiral_model.compile(optimizer=Adam(lr=3.15e-5), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/skimage/exposure/exposure.py:124: UserWarning: This might be a color image. The histogram will be computed on the flattened image. You can instead apply this function to each color channel.\n",
      "  warn(\"This might be a color image. The histogram will be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 40s 483ms/step - loss: 0.9468 - acc: 0.5040 - val_loss: 0.9349 - val_acc: 0.5258\n",
      "Epoch 2/666\n",
      "83/83 [==============================] - 30s 366ms/step - loss: 0.9307 - acc: 0.4960 - val_loss: 0.9199 - val_acc: 0.5041\n",
      "Epoch 3/666\n",
      "83/83 [==============================] - 31s 376ms/step - loss: 0.9169 - acc: 0.4900 - val_loss: 0.9073 - val_acc: 0.5675\n",
      "Epoch 4/666\n",
      "83/83 [==============================] - 30s 367ms/step - loss: 0.9015 - acc: 0.5146 - val_loss: 0.8942 - val_acc: 0.6111\n",
      "Epoch 5/666\n",
      "83/83 [==============================] - 31s 375ms/step - loss: 0.8901 - acc: 0.5000 - val_loss: 0.8824 - val_acc: 0.5417\n",
      "Epoch 6/666\n",
      "83/83 [==============================] - 30s 360ms/step - loss: 0.8786 - acc: 0.5276 - val_loss: 0.8700 - val_acc: 0.5823\n",
      "Epoch 7/666\n",
      "83/83 [==============================] - 31s 372ms/step - loss: 0.8686 - acc: 0.5211 - val_loss: 0.8622 - val_acc: 0.5694\n",
      "Epoch 8/666\n",
      "83/83 [==============================] - 30s 358ms/step - loss: 0.8602 - acc: 0.5236 - val_loss: 0.8489 - val_acc: 0.5679\n",
      "Epoch 9/666\n",
      "83/83 [==============================] - 31s 371ms/step - loss: 0.8464 - acc: 0.5587 - val_loss: 0.8452 - val_acc: 0.5179\n",
      "Epoch 10/666\n",
      "83/83 [==============================] - 30s 356ms/step - loss: 0.8347 - acc: 0.5703 - val_loss: 0.8509 - val_acc: 0.5144\n",
      "Epoch 11/666\n",
      "83/83 [==============================] - 31s 368ms/step - loss: 0.8199 - acc: 0.5949 - val_loss: 0.8288 - val_acc: 0.5675\n",
      "Epoch 12/666\n",
      "83/83 [==============================] - 30s 360ms/step - loss: 0.7992 - acc: 0.6396 - val_loss: 0.8076 - val_acc: 0.6399\n",
      "Epoch 13/666\n",
      "83/83 [==============================] - 30s 366ms/step - loss: 0.7856 - acc: 0.6386 - val_loss: 0.8099 - val_acc: 0.6528\n",
      "Epoch 14/666\n",
      "83/83 [==============================] - 30s 357ms/step - loss: 0.7588 - acc: 0.6682 - val_loss: 0.8035 - val_acc: 0.5988\n",
      "Epoch 15/666\n",
      "83/83 [==============================] - 31s 368ms/step - loss: 0.7574 - acc: 0.6682 - val_loss: 0.7839 - val_acc: 0.5873\n",
      "Epoch 16/666\n",
      "83/83 [==============================] - 29s 351ms/step - loss: 0.7384 - acc: 0.6817 - val_loss: 0.7455 - val_acc: 0.6831\n",
      "Epoch 17/666\n",
      "83/83 [==============================] - 30s 364ms/step - loss: 0.7191 - acc: 0.6978 - val_loss: 0.7517 - val_acc: 0.6607\n",
      "Epoch 18/666\n",
      "83/83 [==============================] - 29s 353ms/step - loss: 0.7102 - acc: 0.7058 - val_loss: 0.7265 - val_acc: 0.6667\n",
      "Epoch 19/666\n",
      "83/83 [==============================] - 30s 364ms/step - loss: 0.6909 - acc: 0.7259 - val_loss: 0.7096 - val_acc: 0.6845\n",
      "Epoch 20/666\n",
      "83/83 [==============================] - 29s 354ms/step - loss: 0.6751 - acc: 0.7239 - val_loss: 0.7194 - val_acc: 0.6687\n",
      "Epoch 21/666\n",
      "83/83 [==============================] - 30s 363ms/step - loss: 0.6736 - acc: 0.7229 - val_loss: 0.6916 - val_acc: 0.7262\n",
      "Epoch 22/666\n",
      "83/83 [==============================] - 29s 354ms/step - loss: 0.6562 - acc: 0.7374 - val_loss: 0.6863 - val_acc: 0.6975\n",
      "Epoch 23/666\n",
      "83/83 [==============================] - 30s 367ms/step - loss: 0.6592 - acc: 0.7239 - val_loss: 0.6416 - val_acc: 0.7579\n",
      "Epoch 24/666\n",
      "83/83 [==============================] - 29s 353ms/step - loss: 0.6396 - acc: 0.7465 - val_loss: 0.6563 - val_acc: 0.7387\n",
      "Epoch 25/666\n",
      "83/83 [==============================] - 30s 365ms/step - loss: 0.6374 - acc: 0.7485 - val_loss: 0.6777 - val_acc: 0.6925\n",
      "Epoch 26/666\n",
      "83/83 [==============================] - 29s 354ms/step - loss: 0.6281 - acc: 0.7505 - val_loss: 0.6710 - val_acc: 0.6872\n",
      "Epoch 27/666\n",
      "83/83 [==============================] - 30s 360ms/step - loss: 0.6218 - acc: 0.7600 - val_loss: 0.6450 - val_acc: 0.7321\n",
      "Epoch 28/666\n",
      "83/83 [==============================] - 29s 350ms/step - loss: 0.6042 - acc: 0.7610 - val_loss: 0.6682 - val_acc: 0.7016\n",
      "Epoch 29/666\n",
      "83/83 [==============================] - 30s 360ms/step - loss: 0.5987 - acc: 0.7636 - val_loss: 0.6636 - val_acc: 0.7143\n",
      "Epoch 30/666\n",
      "83/83 [==============================] - 29s 353ms/step - loss: 0.6054 - acc: 0.7530 - val_loss: 0.6475 - val_acc: 0.7222\n",
      "Epoch 31/666\n",
      "83/83 [==============================] - 30s 360ms/step - loss: 0.5935 - acc: 0.7646 - val_loss: 0.6021 - val_acc: 0.7619\n",
      "Epoch 32/666\n",
      "83/83 [==============================] - 29s 353ms/step - loss: 0.5819 - acc: 0.7666 - val_loss: 0.6701 - val_acc: 0.7016\n",
      "Epoch 33/666\n",
      "83/83 [==============================] - 30s 362ms/step - loss: 0.5712 - acc: 0.7801 - val_loss: 0.5736 - val_acc: 0.7798\n",
      "Epoch 34/666\n",
      "83/83 [==============================] - 29s 349ms/step - loss: 0.5647 - acc: 0.7821 - val_loss: 0.6042 - val_acc: 0.7325\n",
      "Epoch 35/666\n",
      "83/83 [==============================] - 30s 359ms/step - loss: 0.5725 - acc: 0.7761 - val_loss: 0.5665 - val_acc: 0.7698\n",
      "Epoch 36/666\n",
      "83/83 [==============================] - 29s 354ms/step - loss: 0.5501 - acc: 0.7962 - val_loss: 0.5955 - val_acc: 0.7407\n",
      "Epoch 37/666\n",
      "83/83 [==============================] - 30s 359ms/step - loss: 0.5575 - acc: 0.7841 - val_loss: 0.5841 - val_acc: 0.7639\n",
      "Epoch 38/666\n",
      "83/83 [==============================] - 29s 354ms/step - loss: 0.5414 - acc: 0.7947 - val_loss: 0.5811 - val_acc: 0.7613\n",
      "Epoch 39/666\n",
      "83/83 [==============================] - 30s 359ms/step - loss: 0.5415 - acc: 0.7982 - val_loss: 0.5803 - val_acc: 0.7897\n",
      "Epoch 40/666\n",
      "83/83 [==============================] - 30s 356ms/step - loss: 0.5371 - acc: 0.7942 - val_loss: 0.5996 - val_acc: 0.7449\n",
      "Epoch 41/666\n",
      "83/83 [==============================] - 30s 361ms/step - loss: 0.5362 - acc: 0.7917 - val_loss: 0.5525 - val_acc: 0.7778\n",
      "Epoch 42/666\n",
      "83/83 [==============================] - 30s 356ms/step - loss: 0.5091 - acc: 0.8198 - val_loss: 0.5719 - val_acc: 0.7551\n",
      "Epoch 43/666\n",
      "83/83 [==============================] - 30s 358ms/step - loss: 0.5174 - acc: 0.8022 - val_loss: 0.5394 - val_acc: 0.7877\n",
      "Epoch 44/666\n",
      "83/83 [==============================] - 30s 357ms/step - loss: 0.5192 - acc: 0.7982 - val_loss: 0.5960 - val_acc: 0.7428\n",
      "Epoch 45/666\n",
      "83/83 [==============================] - 30s 362ms/step - loss: 0.5032 - acc: 0.7957 - val_loss: 0.5431 - val_acc: 0.7778\n",
      "Epoch 46/666\n",
      "83/83 [==============================] - 30s 359ms/step - loss: 0.5175 - acc: 0.8052 - val_loss: 0.5557 - val_acc: 0.7840\n",
      "Epoch 47/666\n",
      "83/83 [==============================] - 30s 358ms/step - loss: 0.5007 - acc: 0.8173 - val_loss: 0.5487 - val_acc: 0.7817\n",
      "Epoch 48/666\n",
      "83/83 [==============================] - 29s 351ms/step - loss: 0.4926 - acc: 0.8178 - val_loss: 0.5672 - val_acc: 0.7593\n",
      "Epoch 49/666\n",
      "83/83 [==============================] - 30s 359ms/step - loss: 0.4834 - acc: 0.8253 - val_loss: 0.5324 - val_acc: 0.7778\n",
      "Epoch 50/666\n",
      "83/83 [==============================] - 30s 356ms/step - loss: 0.5007 - acc: 0.8047 - val_loss: 0.5371 - val_acc: 0.7634\n",
      "Epoch 51/666\n",
      "83/83 [==============================] - 29s 354ms/step - loss: 0.4850 - acc: 0.8072 - val_loss: 0.5036 - val_acc: 0.8135\n",
      "Epoch 52/666\n",
      "83/83 [==============================] - 29s 355ms/step - loss: 0.4909 - acc: 0.8062 - val_loss: 0.4949 - val_acc: 0.8292\n",
      "Epoch 53/666\n",
      "83/83 [==============================] - 29s 351ms/step - loss: 0.4641 - acc: 0.8313 - val_loss: 0.5221 - val_acc: 0.8016\n",
      "Epoch 54/666\n",
      "83/83 [==============================] - 30s 356ms/step - loss: 0.4649 - acc: 0.8343 - val_loss: 0.5052 - val_acc: 0.8128\n",
      "Epoch 55/666\n",
      "83/83 [==============================] - 29s 354ms/step - loss: 0.4480 - acc: 0.8358 - val_loss: 0.5580 - val_acc: 0.7877\n",
      "Epoch 56/666\n",
      "83/83 [==============================] - 29s 355ms/step - loss: 0.4558 - acc: 0.8298 - val_loss: 0.4980 - val_acc: 0.8086\n",
      "Epoch 57/666\n",
      "83/83 [==============================] - 30s 356ms/step - loss: 0.4506 - acc: 0.8368 - val_loss: 0.4693 - val_acc: 0.8274\n",
      "Epoch 58/666\n",
      "83/83 [==============================] - 30s 359ms/step - loss: 0.4604 - acc: 0.8293 - val_loss: 0.4732 - val_acc: 0.8333\n",
      "Epoch 59/666\n",
      "83/83 [==============================] - 29s 352ms/step - loss: 0.4505 - acc: 0.8434 - val_loss: 0.4860 - val_acc: 0.8254\n",
      "Epoch 60/666\n",
      "83/83 [==============================] - 30s 357ms/step - loss: 0.4424 - acc: 0.8454 - val_loss: 0.4808 - val_acc: 0.8354\n",
      "Epoch 61/666\n",
      "83/83 [==============================] - 29s 352ms/step - loss: 0.4346 - acc: 0.8494 - val_loss: 0.5481 - val_acc: 0.7758\n",
      "Epoch 62/666\n",
      "83/83 [==============================] - 29s 355ms/step - loss: 0.4218 - acc: 0.8534 - val_loss: 0.4621 - val_acc: 0.8251\n",
      "Epoch 63/666\n",
      "83/83 [==============================] - 29s 349ms/step - loss: 0.4323 - acc: 0.8504 - val_loss: 0.4876 - val_acc: 0.8115\n",
      "Epoch 64/666\n",
      "83/83 [==============================] - 29s 355ms/step - loss: 0.4289 - acc: 0.8424 - val_loss: 0.5271 - val_acc: 0.8045\n",
      "Epoch 65/666\n",
      "83/83 [==============================] - 29s 351ms/step - loss: 0.4268 - acc: 0.8534 - val_loss: 0.4842 - val_acc: 0.8115\n",
      "Epoch 66/666\n",
      "11/83 [==>...........................] - ETA: 15s - loss: 0.4316 - acc: 0.8485"
     ]
    }
   ],
   "source": [
    "result=spiral_model.fit_generator(spiral_train_generator,\n",
    "                           validation_data=spiral_val_generator,\n",
    "                           epochs=666,\n",
    "                           steps_per_epoch=(2000//batch_size),\n",
    "                           validation_steps=(800//batch_size),\n",
    "                           callbacks=[reduce_lr,early_stop],\n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's how to load/save\n",
    "spiral_model.save('../nopamine_model_spirals.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network for waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "waves_input_layer (InputLaye (None, 256, 512, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 256, 512, 256)     6656      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 83, 168, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 83, 168, 128)      819328    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 26, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 26, 54, 64)        73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 11, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 11, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 17600)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 666)               11722266  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 666)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 667       \n",
      "=================================================================\n",
      "Total params: 12,622,709\n",
      "Trainable params: 12,622,709\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "waves_model = nopamine_model(mode='waves')\n",
    "waves_model.compile(optimizer=Adam(lr=3.15e-5),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/skimage/exposure/exposure.py:124: UserWarning: This might be a color image. The histogram will be computed on the flattened image. You can instead apply this function to each color channel.\n",
      "  warn(\"This might be a color image. The histogram will be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 62s 753ms/step - loss: 0.9413 - acc: 0.5251 - val_loss: 0.9238 - val_acc: 0.5873\n",
      "Epoch 2/666\n",
      "83/83 [==============================] - 55s 668ms/step - loss: 0.9120 - acc: 0.5663 - val_loss: 0.8721 - val_acc: 0.7593\n",
      "Epoch 3/666\n",
      "83/83 [==============================] - 55s 668ms/step - loss: 0.8568 - acc: 0.6305 - val_loss: 0.7617 - val_acc: 0.7976\n",
      "Epoch 4/666\n",
      "83/83 [==============================] - 55s 665ms/step - loss: 0.7989 - acc: 0.6667 - val_loss: 0.7063 - val_acc: 0.7654\n",
      "Epoch 5/666\n",
      "83/83 [==============================] - 55s 664ms/step - loss: 0.7416 - acc: 0.7118 - val_loss: 0.6396 - val_acc: 0.8115\n",
      "Epoch 6/666\n",
      "83/83 [==============================] - 55s 664ms/step - loss: 0.7283 - acc: 0.7234 - val_loss: 0.6364 - val_acc: 0.8066\n",
      "Epoch 7/666\n",
      "83/83 [==============================] - 55s 666ms/step - loss: 0.6997 - acc: 0.7374 - val_loss: 0.6331 - val_acc: 0.8075\n",
      "Epoch 8/666\n",
      "83/83 [==============================] - 55s 668ms/step - loss: 0.6914 - acc: 0.7339 - val_loss: 0.6406 - val_acc: 0.7860\n",
      "Epoch 9/666\n",
      "83/83 [==============================] - 56s 676ms/step - loss: 0.6820 - acc: 0.7395 - val_loss: 0.6118 - val_acc: 0.7837\n",
      "Epoch 10/666\n",
      "83/83 [==============================] - 55s 666ms/step - loss: 0.6452 - acc: 0.7590 - val_loss: 0.6435 - val_acc: 0.7654\n",
      "Epoch 11/666\n",
      "83/83 [==============================] - 56s 670ms/step - loss: 0.6440 - acc: 0.7610 - val_loss: 0.6017 - val_acc: 0.7976\n",
      "Epoch 12/666\n",
      "83/83 [==============================] - 56s 678ms/step - loss: 0.6297 - acc: 0.7751 - val_loss: 0.6273 - val_acc: 0.7922\n",
      "Epoch 13/666\n",
      "83/83 [==============================] - 56s 673ms/step - loss: 0.6110 - acc: 0.7866 - val_loss: 0.5894 - val_acc: 0.7877\n",
      "Epoch 14/666\n",
      "83/83 [==============================] - 56s 670ms/step - loss: 0.6134 - acc: 0.7771 - val_loss: 0.6037 - val_acc: 0.7942\n",
      "Epoch 15/666\n",
      "83/83 [==============================] - 56s 675ms/step - loss: 0.5860 - acc: 0.7957 - val_loss: 0.6365 - val_acc: 0.7718\n",
      "Epoch 16/666\n",
      "83/83 [==============================] - 55s 668ms/step - loss: 0.5945 - acc: 0.7922 - val_loss: 0.6098 - val_acc: 0.7798\n",
      "Epoch 17/666\n",
      "83/83 [==============================] - 56s 674ms/step - loss: 0.5575 - acc: 0.8238 - val_loss: 0.6078 - val_acc: 0.7857\n",
      "Epoch 18/666\n",
      "83/83 [==============================] - 55s 662ms/step - loss: 0.5760 - acc: 0.7907 - val_loss: 0.6468 - val_acc: 0.7840\n",
      "Epoch 19/666\n",
      "83/83 [==============================] - 55s 662ms/step - loss: 0.5432 - acc: 0.8158 - val_loss: 0.6560 - val_acc: 0.7560\n",
      "Epoch 20/666\n",
      "83/83 [==============================] - 55s 664ms/step - loss: 0.5652 - acc: 0.7912 - val_loss: 0.5652 - val_acc: 0.8045\n",
      "Epoch 21/666\n",
      "83/83 [==============================] - 55s 664ms/step - loss: 0.5343 - acc: 0.8248 - val_loss: 0.6284 - val_acc: 0.7798\n",
      "Epoch 22/666\n",
      "83/83 [==============================] - 55s 667ms/step - loss: 0.5208 - acc: 0.8248 - val_loss: 0.5857 - val_acc: 0.7757\n",
      "Epoch 23/666\n",
      "83/83 [==============================] - 55s 665ms/step - loss: 0.5134 - acc: 0.8394 - val_loss: 0.6237 - val_acc: 0.7659\n",
      "Epoch 24/666\n",
      "83/83 [==============================] - 55s 661ms/step - loss: 0.5183 - acc: 0.8338 - val_loss: 0.6109 - val_acc: 0.7922\n",
      "Epoch 25/666\n",
      "83/83 [==============================] - 55s 668ms/step - loss: 0.5070 - acc: 0.8373 - val_loss: 0.5516 - val_acc: 0.8095\n",
      "Epoch 26/666\n",
      "83/83 [==============================] - 55s 664ms/step - loss: 0.4843 - acc: 0.8389 - val_loss: 0.5804 - val_acc: 0.8025\n",
      "Epoch 27/666\n",
      "83/83 [==============================] - 55s 665ms/step - loss: 0.4788 - acc: 0.8414 - val_loss: 0.5755 - val_acc: 0.7817\n",
      "Epoch 28/666\n",
      "83/83 [==============================] - 55s 665ms/step - loss: 0.5027 - acc: 0.8278 - val_loss: 0.5597 - val_acc: 0.7860\n",
      "Epoch 29/666\n",
      "83/83 [==============================] - 55s 664ms/step - loss: 0.4785 - acc: 0.8494 - val_loss: 0.5230 - val_acc: 0.8214\n",
      "Epoch 30/666\n",
      "83/83 [==============================] - 55s 663ms/step - loss: 0.4805 - acc: 0.8509 - val_loss: 0.5731 - val_acc: 0.8066\n",
      "Epoch 31/666\n",
      "83/83 [==============================] - 55s 661ms/step - loss: 0.4578 - acc: 0.8564 - val_loss: 0.5665 - val_acc: 0.8155\n",
      "Epoch 32/666\n",
      "83/83 [==============================] - 55s 667ms/step - loss: 0.4692 - acc: 0.8599 - val_loss: 0.6341 - val_acc: 0.7942\n",
      "Epoch 33/666\n",
      "83/83 [==============================] - 55s 664ms/step - loss: 0.4571 - acc: 0.8554 - val_loss: 0.5592 - val_acc: 0.8056\n",
      "Epoch 34/666\n",
      "83/83 [==============================] - 55s 663ms/step - loss: 0.4563 - acc: 0.8564 - val_loss: 0.6446 - val_acc: 0.7778\n",
      "Epoch 35/666\n",
      "83/83 [==============================] - 55s 665ms/step - loss: 0.4453 - acc: 0.8670 - val_loss: 0.5912 - val_acc: 0.7917\n",
      "Epoch 36/666\n",
      "83/83 [==============================] - 55s 661ms/step - loss: 0.4296 - acc: 0.8725 - val_loss: 0.6518 - val_acc: 0.7757\n",
      "Epoch 37/666\n",
      "83/83 [==============================] - 56s 669ms/step - loss: 0.4362 - acc: 0.8745 - val_loss: 0.6257 - val_acc: 0.7937\n",
      "Epoch 38/666\n",
      "83/83 [==============================] - 55s 664ms/step - loss: 0.4377 - acc: 0.8705 - val_loss: 0.5652 - val_acc: 0.8066\n",
      "Epoch 39/666\n",
      "83/83 [==============================] - 56s 669ms/step - loss: 0.4346 - acc: 0.8745 - val_loss: 0.5512 - val_acc: 0.8095\n",
      "Epoch 40/666\n",
      "83/83 [==============================] - 55s 667ms/step - loss: 0.4173 - acc: 0.8740 - val_loss: 0.5252 - val_acc: 0.8292\n",
      "Epoch 41/666\n",
      "83/83 [==============================] - 56s 669ms/step - loss: 0.4034 - acc: 0.8901 - val_loss: 0.6406 - val_acc: 0.7897\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 3.149999974993989e-06.\n",
      "Epoch 42/666\n",
      "83/83 [==============================] - 55s 662ms/step - loss: 0.4051 - acc: 0.8815 - val_loss: 0.5523 - val_acc: 0.8230\n",
      "Epoch 43/666\n",
      "83/83 [==============================] - 56s 672ms/step - loss: 0.4092 - acc: 0.8850 - val_loss: 0.5938 - val_acc: 0.7937\n",
      "Epoch 44/666\n",
      "83/83 [==============================] - 55s 661ms/step - loss: 0.3958 - acc: 0.8921 - val_loss: 0.5411 - val_acc: 0.8086\n",
      "Epoch 45/666\n",
      "83/83 [==============================] - 55s 664ms/step - loss: 0.3911 - acc: 0.8966 - val_loss: 0.5564 - val_acc: 0.8313\n",
      "Epoch 00045: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f012b6ddf60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waves_model.fit_generator(wave_train_generator,\n",
    "                          validation_data=wave_val_generator,\n",
    "                          epochs=666,\n",
    "                          steps_per_epoch=(2000//batch_size),\n",
    "                          validation_steps=(800//batch_size),\n",
    "                          callbacks=[reduce_lr,early_stop],\n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "waves_model.save('../nopamine_model_waves.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
